{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNidhkwJb14/YOWw6LAuXgi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaryFriend456/NLP/blob/main/nlplab_p4_22BD1A660W_12_03_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b4eHPXWEJ5O",
        "outputId": "7c999575-38f4-4c0e-85ab-074a0ccb7fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (TF-IDF): 0.6416\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "text1 = \"Machine learning is a field of artificial intelligence.\"\n",
        "text2 = \"Deep learning is a branch of artificial intelligence and machine learning.\"\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "print(f\"Cosine Similarity (TF-IDF): {cosine_sim[0][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvbbXc8OFpRV",
        "outputId": "d74441a8-9223-4286-a762-fa5c77a817ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "def get_embedding(text, model):\n",
        "  words = text.lower().split()\n",
        "  word_vectors = [model[word] for word in words if word in model]\n",
        "  if not word_vectors:\n",
        "    return np.zeros(model.vector_size)\n",
        "  return np.mean(word_vectors, axis=0)\n",
        "\n",
        "embedding1 = get_embedding(text1, glove_model)\n",
        "embedding2 = get_embedding(text2, glove_model)\n",
        "\n",
        "cosine_sim = cosine_similarity([embedding1], [embedding2])\n",
        "print(f\"Cosine Similarity (Word Embeddings - GloVe): {cosine_sim[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0CfoqwJFKwp",
        "outputId": "bfa319ad-176e-4a53-c73a-367abfbc4107"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (Word Embeddings - GloVe): 0.9705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import euclidean, cityblock, hamming\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "euclidean_distance = euclidean(embedding1, embedding2)\n",
        "print(f\"Euclidean Distance: {euclidean_distance:.4f}\")\n",
        "\n",
        "manhattan_distance = cityblock(embedding1, embedding2)\n",
        "print(f\"Manhattan Distance: {manhattan_distance:.4f}\")\n",
        "\n",
        "pearson_correlation, _ = pearsonr(embedding1, embedding2)\n",
        "print(f\"Pearson Correlation: {pearson_correlation:.4f}\")\n",
        "\n",
        "embedding1_binary = np.where(embedding1 > np.mean(embedding1), 1, 0)\n",
        "embedding2_binary = np.where(embedding2 > np.mean(embedding2), 1, 0)\n",
        "\n",
        "min_len = min(len(embedding1_binary), len(embedding2_binary))\n",
        "hamming_distance = hamming(embedding1_binary[:min_len], embedding2_binary[:min_len])\n",
        "\n",
        "\n",
        "print(f\"Hamming Distance: {hamming_distance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVXpgQ8LIjaJ",
        "outputId": "b9c913b7-897b-4681-f3b2-d4f74c824468"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distance: 0.9626\n",
            "Manhattan Distance: 5.4426\n",
            "Pearson Correlation: 0.9703\n",
            "Hamming Distance: 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "count_matrix = vectorizer.fit_transform([text1, text2])\n",
        "jaccard_sim = jaccard_score(count_matrix[0].toarray().flatten(), count_matrix[1].toarray().flatten())\n",
        "print(f\"Jaccard Similarity: {jaccard_sim:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOeIorClK71S",
        "outputId": "e63726b4-beec-4c7a-8394-52c4df3689af"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "tokens1 = preprocess_text(text1)\n",
        "tokens2 = preprocess_text(text2)\n",
        "\n",
        "vocab = set(tokens1 + tokens2)\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "def get_embedding(tokens, vocab_size):\n",
        "  embedding = np.zeros(vocab_size)\n",
        "  for token in tokens:\n",
        "      if token in word_to_index:\n",
        "          embedding[word_to_index[token]] += 1\n",
        "  return embedding\n",
        "\n",
        "embedding1 = get_embedding(tokens1, len(vocab))\n",
        "embedding2 = get_embedding(tokens2, len(vocab))\n",
        "\n",
        "\n",
        "cosine_sim = cosine_similarity([embedding1], [embedding2])\n",
        "print(f\"Cosine Similarity (Word Embeddings - NLTK): {cosine_sim[0][0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eui5HAQGl-o",
        "outputId": "c020829d-083b-4603-d80d-a72cdc4ec617"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (Word Embeddings - NLTK): 0.7454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}